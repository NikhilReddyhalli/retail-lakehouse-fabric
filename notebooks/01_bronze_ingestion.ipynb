{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Synapse PySpark", "language": "Python", "name": "synapse_pyspark"},
  "language_info": {"name": "python"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•â Bronze Layer ‚Äî Raw Data Ingestion\n",
    "**Project:** End-to-End Retail Lakehouse | Microsoft Fabric\n\n",
    "**Layer:** Bronze (Raw / Landing Zone)\n\n",
    "**Purpose:** Ingest raw CSV data as-is into Delta Lake format with metadata columns.\n\n",
    "```\n",
    "Source CSVs ‚Üí Fabric Lakehouse Files ‚Üí Bronze Delta Tables\n",
    "```\n\n",
    "> üìå **Before running:** Upload all CSVs from `data/` folder into your Fabric Lakehouse under `Files/raw_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1 ‚Äî Configuration\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit, input_file_name, col, to_timestamp\n",
    ")\n",
    "import datetime\n",
    "\n",
    "# Your Fabric Lakehouse name ‚Äî update this!\n",
    "LAKEHOUSE_NAME = \"RetailLakehouse\"\n",
    "\n",
    "# Source path (Files section of Lakehouse)\n",
    "RAW_PATH = f\"abfss://your_workspace@onelake.dfs.fabric.microsoft.com/{LAKEHOUSE_NAME}.Lakehouse/Files/raw_data\"\n",
    "\n",
    "# Target path (Tables section ‚Äî Delta Lake)\n",
    "BRONZE_PATH = f\"abfss://your_workspace@onelake.dfs.fabric.microsoft.com/{LAKEHOUSE_NAME}.Lakehouse/Tables/bronze\"\n",
    "\n",
    "# Ingestion run timestamp\n",
    "RUN_TIMESTAMP = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(f\"‚úÖ Config loaded. Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2 ‚Äî Helper Function: Ingest CSV ‚Üí Bronze Delta\n",
    "# ============================================================\n",
    "def ingest_to_bronze(source_file: str, table_name: str, schema=None):\n",
    "    \"\"\"\n",
    "    Reads a CSV from the raw Files zone and writes it to a\n",
    "    Bronze Delta table with metadata columns.\n",
    "    \n",
    "    Args:\n",
    "        source_file: filename in raw_data/ folder\n",
    "        table_name: target Delta table name (bronze_<name>)\n",
    "        schema: optional explicit StructType schema\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Ingesting: {source_file} ‚Üí bronze_{table_name}\")\n",
    "    \n",
    "    # Read CSV\n",
    "    reader = spark.read.option(\"header\", True).option(\"inferSchema\", True)\n",
    "    if schema:\n",
    "        reader = reader.schema(schema)\n",
    "    \n",
    "    df = reader.csv(f\"{RAW_PATH}/{source_file}\")\n",
    "    \n",
    "    # Add Bronze metadata columns\n",
    "    df = df \\\n",
    "        .withColumn(\"_bronze_ingested_at\", current_timestamp()) \\\n",
    "        .withColumn(\"_source_file\", input_file_name()) \\\n",
    "        .withColumn(\"_batch_run_ts\", lit(RUN_TIMESTAMP)) \\\n",
    "        .withColumn(\"_is_deleted\", lit(False))\n",
    "    \n",
    "    row_count = df.count()\n",
    "    print(f\"   Rows read: {row_count:,}\")\n",
    "    print(f\"   Columns:   {len(df.columns)}\")\n",
    "    \n",
    "    # Write to Bronze Delta (overwrite for full load, append for incremental)\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .save(f\"{BRONZE_PATH}/{table_name}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Written to: bronze/{table_name}\")\n",
    "    return row_count\n",
    "\n",
    "print(\"‚úÖ Helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3 ‚Äî Ingest Customers\n",
    "# ============================================================\n",
    "customers_count = ingest_to_bronze(\"customers.csv\", \"customers\")\n",
    "\n",
    "# Preview\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/customers\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4 ‚Äî Ingest Products\n",
    "# ============================================================\n",
    "products_count = ingest_to_bronze(\"products.csv\", \"products\")\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/products\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5 ‚Äî Ingest Stores\n",
    "# ============================================================\n",
    "stores_count = ingest_to_bronze(\"stores.csv\", \"stores\")\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/stores\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6 ‚Äî Ingest Transactions (main fact table)\n",
    "# ============================================================\n",
    "transactions_count = ingest_to_bronze(\"transactions.csv\", \"transactions\")\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/transactions\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7 ‚Äî Bronze Layer Summary\n",
    "# ============================================================\n",
    "print(\"=\" * 50)\n",
    "print(\"ü•â BRONZE LAYER INGESTION COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "summary = [\n",
    "    (\"bronze_customers\",    customers_count),\n",
    "    (\"bronze_products\",     products_count),\n",
    "    (\"bronze_stores\",       stores_count),\n",
    "    (\"bronze_transactions\", transactions_count),\n",
    "]\n",
    "for table, count in summary:\n",
    "    print(f\"  ‚úÖ {table:<30} {count:>10,} rows\")\n",
    "print(f\"\\n  Total rows ingested: {sum(c for _,c in summary):,}\")\n",
    "print(f\"  Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8 ‚Äî Data Quality Check on Bronze\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import count, isnan, when, isnull\n",
    "\n",
    "def null_check(table_name):\n",
    "    df = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/{table_name}\")\n",
    "    data_cols = [c for c in df.columns if not c.startswith(\"_\")]\n",
    "    null_counts = df.select([\n",
    "        count(when(isnull(col(c)), c)).alias(c) for c in data_cols\n",
    "    ])\n",
    "    print(f\"\\nüîç Null check ‚Äî bronze_{table_name}:\")\n",
    "    null_counts.show(truncate=False)\n",
    "\n",
    "null_check(\"transactions\")\n",
    "null_check(\"customers\")"
   ]
  }
 ]
}
