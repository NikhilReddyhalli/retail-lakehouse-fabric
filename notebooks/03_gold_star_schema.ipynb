{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Synapse PySpark", "language": "Python", "name": "synapse_pyspark"},
  "language_info": {"name": "python"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•á Gold Layer ‚Äî Star Schema & SCD Type-2\n",
    "**Project:** End-to-End Retail Lakehouse | Microsoft Fabric\n\n",
    "**Layer:** Gold (Business-Ready / Reporting)\n\n",
    "**Purpose:** Build the final analytics-ready star schema with SCD Type-2 for customer history.\n\n",
    "```\n",
    "Silver Tables ‚Üí Gold Star Schema\n",
    "                ‚îú‚îÄ‚îÄ fact_sales\n",
    "                ‚îú‚îÄ‚îÄ dim_customer   (SCD Type-2)\n",
    "                ‚îú‚îÄ‚îÄ dim_product\n",
    "                ‚îú‚îÄ‚îÄ dim_store\n",
    "                ‚îî‚îÄ‚îÄ dim_date\n",
    "```\n\n",
    "**Star Schema:**\n",
    "```\n",
    "dim_date ‚îÄ‚îÄ‚îê\n",
    "           ‚îú‚îÄ‚îÄ fact_sales ‚îÄ‚îÄ‚îÄ‚îÄ dim_customer (SCD2)\n",
    "dim_store ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ dim_product\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1 ‚Äî Configuration\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, current_date, to_date,\n",
    "    year, month, quarter, dayofweek, dayofmonth, weekofyear,\n",
    "    date_format, when, coalesce, monotonically_increasing_id,\n",
    "    sha2, concat_ws, row_number, max as spark_max, min as spark_min\n",
    ")\n",
    "from pyspark.sql.types import DateType, BooleanType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "\n",
    "LAKEHOUSE_NAME = \"RetailLakehouse\"  # ‚Üê Update!\n",
    "BASE   = f\"abfss://your_workspace@onelake.dfs.fabric.microsoft.com/{LAKEHOUSE_NAME}.Lakehouse/Tables\"\n",
    "SILVER = f\"{BASE}/silver\"\n",
    "GOLD   = f\"{BASE}/gold\"\n",
    "\n",
    "SCD2_ACTIVE_END_DATE = \"9999-12-31\"  # Sentinel date for active records\n",
    "\n",
    "print(\"‚úÖ Gold layer config ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2 ‚Äî dim_date (Calendar Dimension)\n",
    "# ============================================================\n",
    "print(\"\\nüìÖ Building dim_date...\")\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "# Generate date range 2020-01-01 to 2026-12-31\n",
    "start = datetime.date(2020, 1, 1)\n",
    "end   = datetime.date(2026, 12, 31)\n",
    "date_list = [start + datetime.timedelta(days=i) for i in range((end - start).days + 1)]\n",
    "\n",
    "date_rows = [Row(full_date=d.strftime(\"%Y-%m-%d\")) for d in date_list]\n",
    "df_dates = spark.createDataFrame(date_rows)\n",
    "\n",
    "dim_date = df_dates \\\n",
    "    .withColumn(\"full_date\",     to_date(col(\"full_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"date_key\",      date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\",          year(col(\"full_date\"))) \\\n",
    "    .withColumn(\"quarter\",       quarter(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month_num\",     month(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month_name\",    date_format(col(\"full_date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"month_short\",   date_format(col(\"full_date\"), \"MMM\")) \\\n",
    "    .withColumn(\"week_of_year\",  weekofyear(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_of_month\",  dayofmonth(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_of_week\",   dayofweek(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_name\",      date_format(col(\"full_date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"is_weekend\",    col(\"day_of_week\").isin([1, 7])) \\\n",
    "    .withColumn(\"year_month\",    date_format(col(\"full_date\"), \"yyyy-MM\")) \\\n",
    "    .withColumn(\"year_quarter\",  concat_ws(\"-Q\", col(\"year\"), col(\"quarter\")))\n",
    "\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\").save(f\"{GOLD}/dim_date\")\n",
    "\n",
    "print(f\"   ‚úÖ dim_date written ‚Äî {dim_date.count():,} rows\")\n",
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3 ‚Äî dim_product\n",
    "# ============================================================\n",
    "print(\"\\nüì¶ Building dim_product...\")\n",
    "\n",
    "silver_products = spark.read.format(\"delta\").load(f\"{SILVER}/products\")\n",
    "\n",
    "dim_product = silver_products \\\n",
    "    .withColumn(\"product_key\",\n",
    "        sha2(col(\"product_id\"), 256).substr(1, 16)\n",
    "    ) \\\n",
    "    .withColumn(\"dw_created_at\", current_timestamp()) \\\n",
    "    .select(\n",
    "        \"product_key\", \"product_id\", \"product_name\", \"category\",\n",
    "        \"sub_category\", \"unit_price\", \"cost_price\", \"gross_margin_pct\",\n",
    "        \"price_tier\", \"supplier\", \"in_stock\", \"dw_created_at\"\n",
    "    )\n",
    "\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\").save(f\"{GOLD}/dim_product\")\n",
    "\n",
    "print(f\"   ‚úÖ dim_product written ‚Äî {dim_product.count():,} rows\")\n",
    "dim_product.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4 ‚Äî dim_store\n",
    "# ============================================================\n",
    "print(\"\\nüè™ Building dim_store...\")\n",
    "\n",
    "silver_stores = spark.read.format(\"delta\").load(f\"{SILVER}/stores\")\n",
    "\n",
    "dim_store = silver_stores \\\n",
    "    .withColumn(\"store_key\", sha2(col(\"store_id\"), 256).substr(1, 16)) \\\n",
    "    .withColumn(\"dw_created_at\", current_timestamp()) \\\n",
    "    .select(\n",
    "        \"store_key\", \"store_id\", \"store_name\", \"store_type\",\n",
    "        \"city\", \"region\", \"open_date\", \"store_age_days\",\n",
    "        \"is_online\", \"dw_created_at\"\n",
    "    )\n",
    "\n",
    "dim_store.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\").save(f\"{GOLD}/dim_store\")\n",
    "\n",
    "print(f\"   ‚úÖ dim_store written ‚Äî {dim_store.count():,} rows\")\n",
    "dim_store.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5 ‚Äî dim_customer with SCD Type-2\n",
    "# ============================================================\n",
    "print(\"\\nüë§ Building dim_customer (SCD Type-2)...\")\n",
    "\n",
    "# SCD Type-2 tracks historical changes in customer attributes\n",
    "# Each row has: effective_start_date, effective_end_date, is_current flag\n",
    "\n",
    "silver_customers = spark.read.format(\"delta\").load(f\"{SILVER}/customers\")\n",
    "\n",
    "# ‚îÄ‚îÄ Initial load (first time): create SCD2 structure ‚îÄ‚îÄ\n",
    "def initial_scd2_load(df):\n",
    "    return df \\\n",
    "        .withColumn(\"customer_key\",\n",
    "            sha2(concat_ws(\"|\", col(\"customer_id\"), col(\"_silver_updated_at\")), 256).substr(1, 16)\n",
    "        ) \\\n",
    "        .withColumn(\"effective_start_date\", current_date()) \\\n",
    "        .withColumn(\"effective_end_date\",   to_date(lit(SCD2_ACTIVE_END_DATE), \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"is_current\",           lit(True)) \\\n",
    "        .withColumn(\"dw_created_at\",        current_timestamp()) \\\n",
    "        .withColumn(\"dw_updated_at\",        current_timestamp()) \\\n",
    "        .select(\n",
    "            \"customer_key\", \"customer_id\", \"full_name\", \"first_name\", \"last_name\",\n",
    "            \"email\", \"email_domain\", \"city\", \"segment\",\n",
    "            \"signup_date\", \"is_active\", \"customer_tenure_days\",\n",
    "            \"effective_start_date\", \"effective_end_date\", \"is_current\",\n",
    "            \"dw_created_at\", \"dw_updated_at\"\n",
    "        )\n",
    "\n",
    "dim_customer = initial_scd2_load(silver_customers)\n",
    "\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\").save(f\"{GOLD}/dim_customer\")\n",
    "\n",
    "print(f\"   ‚úÖ dim_customer initial load ‚Äî {dim_customer.count():,} rows\")\n",
    "dim_customer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6 ‚Äî SCD Type-2 MERGE (for incremental updates)\n",
    "# ============================================================\n",
    "# Run this cell whenever customers_updated.csv is ingested\n",
    "print(\"\\nüîÑ Running SCD Type-2 MERGE for customer updates...\")\n",
    "\n",
    "# Read the updated customer records (after Bronze+Silver processing)\n",
    "# In production: this comes from your Silver customers table after new batch\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Simulating: read updated customers from Silver\n",
    "# In real run: replace this path with actual updated silver data path\n",
    "updated_customers = silver_customers  # placeholder - in prod this is the new batch\n",
    "\n",
    "# Load existing Gold dim_customer\n",
    "dim_customer_delta = DeltaTable.forPath(spark, f\"{GOLD}/dim_customer\")\n",
    "\n",
    "# SCD Type-2 logic using Delta MERGE:\n",
    "# 1. For changed records: expire old row (set is_current=False, update end_date)\n",
    "# 2. Insert new rows for changed records with is_current=True\n",
    "\n",
    "# Step 1: Find changed records (compare SCD columns: city, segment)\n",
    "SCD2_COLUMNS = [\"city\", \"segment\", \"is_active\"]\n",
    "\n",
    "changed = updated_customers.alias(\"new\").join(\n",
    "    dim_customer_delta.toDF().filter(col(\"is_current\") == True).alias(\"existing\"),\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ").filter(\n",
    "    # Detect any change in tracked columns\n",
    "    \" OR \".join([f\"new.{c} != existing.{c}\" for c in SCD2_COLUMNS])\n",
    ")\n",
    "\n",
    "changed_ids = [r[\"customer_id\"] for r in changed.select(\"customer_id\").collect()]\n",
    "print(f\"   Changed customers detected: {len(changed_ids)}\")\n",
    "\n",
    "# Step 2: Expire old rows via MERGE\n",
    "dim_customer_delta.alias(\"target\").merge(\n",
    "    updated_customers.filter(col(\"customer_id\").isin(changed_ids)).alias(\"source\"),\n",
    "    condition=\"target.customer_id = source.customer_id AND target.is_current = true\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"is_current\":         lit(False),\n",
    "    \"effective_end_date\": current_date(),\n",
    "    \"dw_updated_at\":      current_timestamp()\n",
    "}).execute()\n",
    "\n",
    "# Step 3: Insert new rows for changed customers\n",
    "new_rows = initial_scd2_load(\n",
    "    updated_customers.filter(col(\"customer_id\").isin(changed_ids))\n",
    ")\n",
    "new_rows.write.format(\"delta\").mode(\"append\").save(f\"{GOLD}/dim_customer\")\n",
    "\n",
    "print(f\"   ‚úÖ SCD Type-2 merge complete ‚Äî {len(changed_ids)} records updated\")\n",
    "\n",
    "# Verify ‚Äî show a changed customer with history\n",
    "if changed_ids:\n",
    "    sample_id = changed_ids[0]\n",
    "    print(f\"\\n   Sample SCD2 history for customer_id = {sample_id}:\")\n",
    "    spark.read.format(\"delta\").load(f\"{GOLD}/dim_customer\") \\\n",
    "        .filter(col(\"customer_id\") == sample_id) \\\n",
    "        .select(\"customer_id\",\"city\",\"segment\",\"effective_start_date\",\"effective_end_date\",\"is_current\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7 ‚Äî fact_sales (Core Fact Table)\n",
    "# ============================================================\n",
    "print(\"\\nüí∞ Building fact_sales...\")\n",
    "\n",
    "silver_txn      = spark.read.format(\"delta\").load(f\"{SILVER}/transactions\")\n",
    "dim_customer_df = spark.read.format(\"delta\").load(f\"{GOLD}/dim_customer\").filter(col(\"is_current\") == True)\n",
    "dim_product_df  = spark.read.format(\"delta\").load(f\"{GOLD}/dim_product\")\n",
    "dim_store_df    = spark.read.format(\"delta\").load(f\"{GOLD}/dim_store\")\n",
    "dim_date_df     = spark.read.format(\"delta\").load(f\"{GOLD}/dim_date\")\n",
    "\n",
    "fact_sales = silver_txn \\\n",
    "    .join(\n",
    "        dim_customer_df.select(\"customer_key\", \"customer_id\"),\n",
    "        on=\"customer_id\", how=\"left\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        dim_product_df.select(\"product_key\", \"product_id\"),\n",
    "        on=\"product_id\", how=\"left\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        dim_store_df.select(\"store_key\", \"store_id\"),\n",
    "        on=\"store_id\", how=\"left\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        dim_date_df.select(\"full_date\", \"date_key\"),\n",
    "        silver_txn[\"transaction_date\"] == dim_date_df[\"full_date\"],\n",
    "        how=\"left\"\n",
    "    ) \\\n",
    "    .withColumn(\"sales_key\",\n",
    "        sha2(concat_ws(\"|\", col(\"transaction_id\"), col(\"transaction_date\")), 256).substr(1,16)\n",
    "    ) \\\n",
    "    .withColumn(\"dw_created_at\", current_timestamp()) \\\n",
    "    .select(\n",
    "        # Keys\n",
    "        \"sales_key\", \"transaction_id\",\n",
    "        \"customer_key\", \"product_key\", \"store_key\", \"date_key\",\n",
    "        # Dimensions (degenerate)\n",
    "        \"transaction_date\", \"txn_year\", \"txn_month\", \"txn_quarter\",\n",
    "        \"payment_method\", \"source_system\", \"status\", \"is_returned\",\n",
    "        # Measures\n",
    "        \"quantity\",\n",
    "        \"unit_price\",\n",
    "        \"discount_pct\",\n",
    "        \"discount_amount\",\n",
    "        \"gross_revenue\",\n",
    "        \"total_amount\",\n",
    "        # Metadata\n",
    "        \"dw_created_at\"\n",
    "    )\n",
    "\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\").save(f\"{GOLD}/fact_sales\")\n",
    "\n",
    "print(f\"   ‚úÖ fact_sales written ‚Äî {fact_sales.count():,} rows\")\n",
    "fact_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8 ‚Äî Gold Layer Validation & Business Metrics\n",
    "# ============================================================\n",
    "from pyspark.sql.functions import spark_sum, avg, countDistinct\n",
    "\n",
    "fact = spark.read.format(\"delta\").load(f\"{GOLD}/fact_sales\")\n",
    "\n",
    "print(\"\\nüìä Gold Layer Summary\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Table counts\n",
    "for table in [\"fact_sales\", \"dim_customer\", \"dim_product\", \"dim_store\", \"dim_date\"]:\n",
    "    df = spark.read.format(\"delta\").load(f\"{GOLD}/{table}\")\n",
    "    print(f\"  ü•á {table:<22} {df.count():>10,} rows\")\n",
    "\n",
    "print(\"\\nüí∞ Key Business Metrics (from fact_sales):\")\n",
    "metrics = fact.filter(col(\"is_returned\") == False).agg(\n",
    "    spark_sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    spark_sum(\"quantity\").alias(\"total_units_sold\"),\n",
    "    countDistinct(\"customer_key\").alias(\"unique_customers\"),\n",
    "    countDistinct(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    spark_sum(\"discount_amount\").alias(\"total_discounts\")\n",
    ")\n",
    "metrics.show(truncate=False)\n",
    "\n",
    "print(\"\\nüìà Revenue by Quarter:\")\n",
    "fact.filter(col(\"is_returned\") == False) \\\n",
    "    .groupBy(\"txn_year\", \"txn_quarter\") \\\n",
    "    .agg(spark_sum(\"total_amount\").alias(\"revenue\")) \\\n",
    "    .orderBy(\"txn_year\", \"txn_quarter\") \\\n",
    "    .show()"
   ]
  }
 ]
}
